# ğŸš€ lbrxVoice MVP - Ready for Integration

**Status:** âœ… MVP Ready  
**Data:** 2025-05-30  
**Pipeline:** Audio â†’ ASR â†’ LLM â†’ TTS â†’ Audio

## ğŸ“‹ Co dostarcza MVP

### 1. **Kompletny pipeline gÅ‚osowy**
```python
# Prosty w uÅ¼yciu
from lbrx_voice_pipeline import VoicePipeline

pipeline = VoicePipeline()
results = await pipeline.process_audio("wizyta.m4a")
```

### 2. **Komponenty**

#### ASR (Speech-to-Text)
- âœ… **MLX Whisper** - natywne dla Apple Silicon
- âœ… Optymalizacja dla jÄ™zyka polskiego
- âœ… Zapobieganie powtÃ³rzeniom (`condition_on_previous_text=False`)
- âœ… API REST (port 8123) i WebSocket (port 8126)

#### LLM (Analiza tekstu)
- âœ… **Qwen3-8B** przez LM Studio
- âœ… Analiza wizyt weterynaryjnych
- âœ… Ekstrakcja kluczowych informacji
- âœ… Format JSON

#### TTS (Text-to-Speech)
- âœ… **MLX-Audio** z modelami Kokoro/Spark
- âœ… Wsparcie dla jÄ™zyka polskiego
- âœ… Generowanie odpowiedzi gÅ‚osowych

## ğŸ”Œ Endpointy API

### Batch Transcription
```bash
curl -X POST http://localhost:8123/v1/audio/transcriptions \
  -F "file=@audio.m4a" \
  -F "language=pl" \
  -F "response_format=json"
```

### Realtime WebSocket
```javascript
const ws = new WebSocket('ws://localhost:8126/v1/audio/transcriptions');
ws.send(base64AudioData);
```

### LLM Analysis
```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3-8b-mlx", "messages": [...]}'
```

## ğŸ—ï¸ Integracja z Tauri/Vista

### 1. **Backend API**
```typescript
// W Tauri - wywoÅ‚aj backend
const response = await fetch('http://localhost:8123/v1/audio/transcriptions', {
  method: 'POST',
  body: formData
});
```

### 2. **Streaming**
```typescript
// WebSocket dla real-time
const ws = new WebSocket('ws://localhost:8126/v1/audio/transcriptions');
ws.onmessage = (event) => {
  const transcription = JSON.parse(event.data);
  updateUI(transcription);
};
```

### 3. **Pipeline kompletny**
```typescript
// Lub uÅ¼yj pipeline endpoint (TODO)
const result = await processVetVisit(audioBlob);
// result = { transcription, analysis, audioResponse }
```

## ğŸš€ Uruchomienie

### 1. Serwery
```bash
# Terminal 1 - Serwery Whisper
python start_servers.py

# Terminal 2 - LM Studio
# Uruchom LM Studio z modelem qwen3-8b-mlx
```

### 2. Test pipeline
```bash
python lbrx_voice_pipeline.py
```

### 3. Konfiguracja
```bash
# Interaktywny konfigurator
python tools/whisper_config_tui.py
```

## ğŸ“Š PrzykÅ‚ad wyniku

```json
{
  "transcription": {
    "text": "Pies ma gorÄ…czkÄ™ 40 stopni, wymiotuje od wczoraj...",
    "language": "pl"
  },
  "analysis": {
    "objawy": ["gorÄ…czka 40Â°C", "wymioty"],
    "diagnoza": "Podejrzenie zatrucia pokarmowego",
    "leczenie": "PÅ‚yny doÅ¼ylne, Cerenia",
    "zalecenia": "Dieta, obserwacja 24h"
  },
  "audio_response": "outputs/response.wav"
}
```

## ğŸ”§ Konfiguracja

### Whisper (ASR)
- Model: `whisper-large-v3-mlx`
- Optymalizacja: Polski preset
- Beam size: 5
- No repetition: `condition_on_previous_text=False`

### LLM
- Model: `qwen3-8b-mlx`
- Endpoint: `http://localhost:1234`
- Temperature: 0.3 (dla spÃ³jnoÅ›ci)

### TTS
- Model: `kokoro` (multilingual)
- Alternative: `spark` (English)
- Format: WAV 16kHz

## ğŸ“ Struktura projektu

```
lbrxWhisper/
â”œâ”€â”€ lbrx_voice_pipeline.py    # MVP pipeline
â”œâ”€â”€ start_servers.py          # Uruchom serwery
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ whisper_config.py     # Konfiguracja
â”‚   â””â”€â”€ whisper_config_tui.py # Konfigurator TUI
â”œâ”€â”€ whisper_servers/          # Serwery API
â”‚   â”œâ”€â”€ batch/               # REST API
â”‚   â””â”€â”€ realtime/            # WebSocket
â””â”€â”€ outputs/                 # Wyniki
```

## âš ï¸ Znane ograniczenia

1. **TTS** - MLX-Audio w wersji beta, moÅ¼liwe problemy z polskim
2. **LLM** - Wymaga dziaÅ‚ajÄ…cego LM Studio
3. **PamiÄ™Ä‡** - ~6GB RAM dla peÅ‚nego pipeline

## ğŸ¯ NastÄ™pne kroki

1. **Docker** - Konteneryzacja dla Å‚atwego deploymentu
2. **Queue** - System kolejkowania dla wielu uÅ¼ytkownikÃ³w  
3. **Cache** - Buforowanie wynikÃ³w
4. **Fine-tuning** - Dostosowanie modeli do weterynarii

---

**Gotowe do integracji z Vista!** ğŸš€

Kontakt: Maciej Gad @ LibraxisAI